{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bddfae3-6227-461d-8fa3-0343779225b6",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaca4fa-6c25-4d00-895d-ff04fecd6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a634b-8abc-44f8-ae55-1bbc265e6444",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d793b07-7ac9-4146-b855-86bf0a7d22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "file_path = r'C:\\Users\\File.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data.dropna(subset=['Org'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2472db-a403-4822-90f4-5a7e490a5fc3",
   "metadata": {},
   "source": [
    "# Data Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d2a13-4b05-47ee-a3f0-91c1f7c60c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each unique 'Source' IP to a Server ID\n",
    "unique_servers = data['Source'].unique()\n",
    "server_mapping = {ip: idx for idx, ip in enumerate(unique_servers)}\n",
    "data['ServerID'] = data['Source'].map(server_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbd0fd-80ab-40e3-939c-afda2b49e2e2",
   "metadata": {},
   "source": [
    "# Sort & Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397c095-64c7-4346-9af0-57881b3c221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert AdjustedTime to numeric format\n",
    "data['AdjustedTime'] = pd.to_datetime(data['AdjustedTime']).astype('int64') // 10**9  # Convert to seconds\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "categorical_cols = ['Protocol', 'Connection', 'User', 'Org']\n",
    "for col in categorical_cols:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    data[col] = label_encoders[col].fit_transform(data[col])\n",
    "\n",
    "# Define feature and target columns\n",
    "features = ['AdjustedTime', 'Protocol', 'Connection', 'User', 'Length', 'ARTT', 'Longitude', 'Latitude', 'Org']\n",
    "labels = 'ServerID'\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Group by user to create sequences\n",
    "user_groups = data.groupby('User')\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "sequence_length = 60\n",
    "\n",
    "for user, group in user_groups:\n",
    "    user_features = group[features].values\n",
    "    user_labels = group[labels].values\n",
    "    \n",
    "    # Create rolling sequences\n",
    "    for i in range(len(user_features) - sequence_length + 1):\n",
    "        X_sequences.append(user_features[i:i + sequence_length])\n",
    "        y_sequences.append(user_labels[i + sequence_length - 1])  # Target is last ID in the sequence\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_sequences = pad_sequences(X_sequences, maxlen=sequence_length, dtype='float32')\n",
    "y_sequences = np.array(y_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176d753-2f08-461a-96a4-992eedad9ea8",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3de154-8202-415e-98d4-bd7862a7f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e88221-ed2c-452f-990f-68f48a2cc3b0",
   "metadata": {},
   "source": [
    "# Frequency Enhanced Decomposition Transformer Model Defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44bf0b-bb3e-4aa9-9d7e-14dc938fc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Conv1D, Add, GlobalAveragePooling1D, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.signal as tf_signal\n",
    "\n",
    "# Fourier Transform Layer\n",
    "class FourierTransformLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # Compute Fourier Transform (FFT) along the time axis\n",
    "        fft = tf_signal.fft(tf.cast(inputs, dtype=tf.complex64))\n",
    "        # Take only real part\n",
    "        return tf.math.real(fft)\n",
    "\n",
    "# FEDformer Encoder Block\n",
    "class FEDformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(FEDformerEncoder, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.decomp = Conv1D(filters=embed_dim, kernel_size=3, padding=\"same\", activation=\"linear\")  # Trend decomposition\n",
    "        self.fourier = FourierTransformLayer()  # Frequency-based decomposition\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Time-series decomposition (trend extraction)\n",
    "        trend = self.decomp(inputs)  # Trend component\n",
    "        seasonal = inputs - trend  # Seasonal component\n",
    "\n",
    "        # Frequency decomposition\n",
    "        freq_output = self.fourier(seasonal)\n",
    "\n",
    "        # Attention on frequency-enhanced seasonal component\n",
    "        attn_output = self.attention(freq_output, freq_output)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.norm1(seasonal + attn_output)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "        # Combine trend, seasonal, and frequency components\n",
    "        return self.norm2(trend + ffn_output)\n",
    "\n",
    "# Model Architecture\n",
    "sequence_length = X_train.shape[1]\n",
    "feature_dim = X_train.shape[2]\n",
    "num_classes = len(unique_servers)\n",
    "\n",
    "inputs = Input(shape=(sequence_length, feature_dim))\n",
    "\n",
    "# Stack FEDformer Encoder Blocks\n",
    "x = FEDformerEncoder(embed_dim=feature_dim, num_heads=4, ff_dim=128)(inputs)\n",
    "x = FEDformerEncoder(embed_dim=feature_dim, num_heads=4, ff_dim=128)(x)\n",
    "x = FEDformerEncoder(embed_dim=feature_dim, num_heads=4, ff_dim=128)(x)\n",
    "\n",
    "# Global Average Pooling\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Fully connected layers\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define and compile the model\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82d7a9-04e4-4e32-b8cc-68372424a138",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12b614-2d92-45cd-9dfb-9e91ef825c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_split=0.176, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3463ffa1-5d0b-4cd7-9bb7-41b9fbfad0d6",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063064fc-d096-46f4-82f1-13ff72ed9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Model accuracy on test set:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
